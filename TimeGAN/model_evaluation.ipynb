{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "import neptune.new as neptune\n",
    "from trainers import timegan_generate_data, rtsgan_generator, rgan_generator\n",
    "from utils import *\n",
    "\n",
    "# Makes sure the same testset is generated every time\n",
    "np.random.seed(42)\n",
    "alpha = 0.7\n",
    "noise = 0\n",
    "testset = DatasetSinus(num=600, seq_len=100, alpha=alpha, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(testset[:][0][2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# Models\n",
    "from architectures.timegan_cnn_DG import *\n",
    "#from architectures.timegan_cnn_D import *\n",
    "#from architectures.timegan_cnn_G import *\n",
    "#from architectures.timegan_cnn_DGER import *\n",
    "#from architectures.RGAN import *\n",
    "#from architectures.RTSGAN import *\n",
    "\n",
    "print(f\"Loading architecture: {ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id = \"TIMEGAN-178\" # RTSGAN-66\n",
    "#id = \"RTSGAN-66\"\n",
    "project_name = \"timeGAN\" # RTSGAN\n",
    "#project_name = \"RTSGAN\"\n",
    "\n",
    "run = neptune.init_run(\n",
    "                with_id=id, # \"TIMEGAN-84\"\n",
    "                project=\"kohmann/\" + project_name,\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3YjFjNGY5MS1kOWU1LTRmZjgtOTNiYS0yOGI2NDdjZGYzNWUifQ==\",\n",
    "                   )\n",
    "params = run[\"parameters\"].fetch()\n",
    "params[\"device\"] = \"cpu\"\n",
    "params[\"testset_size\"] = len(testset)\n",
    "#params[\"model_name\"] = \"model_checkpoint.pt\"\n",
    "\n",
    "if \"TimeGAN\" in ID:\n",
    "    model = TimeGAN(params)\n",
    "elif ID == \"RGAN\":\n",
    "    model = RGAN(params)\n",
    "elif ID == \"RTSGAN\":\n",
    "    model = RTSGAN(params)\n",
    "else:\n",
    "    raise ValueError\n",
    "model = restore_weights(model, run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "np.random.seed(42)\n",
    "#fake_data = rtsgan_generator(model, params, eval=True)\n",
    "fake_data = timegan_generate_data(model, torch.tensor(testset.T), params[\"max_seq_len\"], params[\"Z_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"kohmann/Evaluation\",\n",
    "    name=ID,\n",
    "    description=\"\",\n",
    "    #source_files=[\"architectures/RTSGAN.py\"],\n",
    "    capture_hardware_metrics=False,\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3YjFjNGY5MS1kOWU1LTRmZjgtOTNiYS0yOGI2NDdjZGYzNWUifQ==\",\n",
    ")\n",
    "run[\"model_id\"] = ID + \"-\" + id.split('-')[-1]\n",
    "\n",
    "from metrics import compare_sin3_generation, sw_approx # sinkhorn_distance, MMD,\n",
    "np.random.seed(42)\n",
    "testset2 = DatasetSinus(num=1000, seq_len=100, alpha=alpha, noise=noise)\n",
    "mse_error = compare_sin3_generation(fake_data, 0.7, 0)\n",
    "print(f\"MSE Error: {mse_error:.5f}\")\n",
    "x = torch.tensor(fake_data)\n",
    "y = testset[:][0]\n",
    "y_2 = testset2[:][0]\n",
    "#wass_dist = sinkhorn_distance(x,y)\n",
    "#mmd = MMD(x,y)\n",
    "sw_baseline = sw_approx(y,y_2)\n",
    "sw = sw_approx(y,x)\n",
    "\n",
    "run[\"numeric_results/num_test_samples\"] = len(testset)\n",
    "run[\"numeric_results/sin3_generation_MSE_loss\"] = mse_error\n",
    "#run[\"numeric_results/wasserstein_distance_mean\"] = wass_dist.mean()\n",
    "#run[\"numeric_results/wasserstein_distance_std\"] = wass_dist.std()\n",
    "run[\"numeric_results/SW\"] = sw.item()\n",
    "run[\"numeric_results/SW_baseline\"] = sw_baseline.item()\n",
    "\n",
    "r = np.array([data[0].numpy() for data in testset])\n",
    "#sin = None\n",
    "#f_pca = visualization(r[:,:,sin][:,:,None], fake_data[:,:,sin][:,:,None], 'umap')\n",
    "run[\"PCA\"].upload(visualization(r, fake_data, 'pca'))\n",
    "run[\"tSNE\"].upload(visualization(r, fake_data, 'tsne'))\n",
    "run[\"UMAP\"].upload(visualization(r, fake_data, 'umap'))\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from metrics import compare_sin3_generation, sinkhorn_distance, MMD\n",
    "mse_error = compare_sin3_generation(fake_data, 0.7, 0)\n",
    "print(f\"MSE Error: {mse_error:.5f}\")\n",
    "x = torch.tensor(fake_data, dtype=torch.float32)\n",
    "y = testset[:][0]\n",
    "wass_dist = sinkhorn_distance(x,y, blur=0.01)\n",
    "mmd = MMD(x,y)\n",
    "print(f\"Mean Wasserstein/Sinkhorn distance: {wass_dist.mean():.8f} ± {wass_dist.std():.8f}\")\n",
    "print(f\"Mean MMD: {mmd.mean():.6f} ± {mmd.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wass_dist.mean().numpy(), wass_dist.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(fake_data, dtype=torch.float32)[0]\n",
    "y = testset[:][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_approx(mu: torch.Tensor, nu: torch.Tensor) -> float:\n",
    "\n",
    "    def m_2(X):\n",
    "        return torch.mean(torch.pow(X, 2), dim=0)\n",
    "\n",
    "    m_mu = torch.mean(mu, dim=0)\n",
    "    m_nu = torch.mean(nu, dim=0)\n",
    "    ### First lets compute d:=W2{N(0, m2(µd_bar)), N(0, m2(νd_bar))}\n",
    "    # Centered version of mu and nu\n",
    "    mu_bar = mu - m_mu\n",
    "    nu_bar = nu - m_nu\n",
    "    # Compute Wasserstein beetween two centered gaussians\n",
    "    W = torch.pow(torch.sqrt(m_2(mu_bar)) - torch.sqrt(m_2(nu_bar)), 2)\n",
    "\n",
    "    ## Compute the mean residuals\n",
    "    d = mu.size(1)\n",
    "    res = (1 / d) * torch.pow(m_mu - m_nu, 2)\n",
    "\n",
    "    ## Approximation of the Sliced Wasserstein\n",
    "    return torch.norm(W + res, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "N = 10**4\n",
    "testset1 = DatasetSinus(num=N, seq_len=100, alpha=0.7, noise=0)[:][0]\n",
    "testset2 = DatasetSinus(num=N, seq_len=100, alpha=0.7, noise=0)[:][0]\n",
    "plt.plot(testset1[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = [sw_approx(testset1[:10**n].reshape(-1, 3*100), testset2[:10**n].reshape(-1, 3*100)).item() for n in range(1, 5)]\n",
    "plt.plot([10**n for n in range(1, 5)], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_approx(testset1[:1000], testset2[:1000]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[10**n for n in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import time\n",
    "\n",
    "## First sample from two different distributions\n",
    "m1 = torch.tensor([1., 2., 3.])\n",
    "m2 = torch.tensor([4., 5., 6.])\n",
    "sig1 = torch.tensor([[1., 1., 1.], [1., 2., 2.], [1., 2., 3.]])\n",
    "sig2 = torch.eye(3)\n",
    "mu_distrib = MultivariateNormal(m1, sig1)\n",
    "nu_distrib = MultivariateNormal(m2, sig2)\n",
    "\n",
    "n = 10000 # number of samples\n",
    "mu_samples = mu_distrib.rsample(sample_shape=torch.Size([n]))\n",
    "nu_samples = nu_distrib.rsample(sample_shape=torch.Size([n]))\n",
    "\n",
    "# True Wasserstein\n",
    "w = torch.norm(m1 - m2, p=2) + torch.trace(sig1 + sig2 - 2*torch.sqrt(torch.sqrt(sig1) * sig2 * torch.sqrt(sig1)))\n",
    "print(\"true Wasserstein  :\", w)\n",
    "\n",
    "# Approximation of the Sliced Wasserstein\n",
    "start = time.time()\n",
    "sw_ap = sw_approx(mu_samples, nu_samples)\n",
    "print(f\"Approx SW : {sw_ap} ----- time : {time.time() - start} ---- approx error {torch.abs(sw_ap - w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_samples.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cost_xy(x, y, scaling_coef):\n",
    "    '''\n",
    "    L2 distance between vectors, using expanding and hence is more memory intensive\n",
    "    :param x: x is tensor of shape [batch_size, time steps, features]\n",
    "    :param y: y is tensor of shape [batch_size, time steps, features]\n",
    "    :param scaling_coef: a scaling coefficient for distance between x and y\n",
    "    :return: cost matrix: a matrix of size [batch_size, batch_size] where\n",
    "    '''\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    y = torch.unsqueeze(y, 0)\n",
    "    sum_over_pixs = torch.sum((x - y)**2, -1) * scaling_coef\n",
    "    sum_over_time = torch.sum(sum_over_pixs, -1)\n",
    "    return sum_over_time\n",
    "\n",
    "def benchmark_sinkhorn(x, y, scaling_coef, epsilon=1.0, L=10, Lmin=10):\n",
    "    '''\n",
    "    Given two emprical measures with n points each with locations x and y\n",
    "    outputs an approximation of the OT cost with regularization parameter epsilon\n",
    "    niter is the max. number of steps in sinkhorn loop\n",
    "    '''\n",
    "    n_data = x.shape[0]\n",
    "\n",
    "    # The Sinkhorn algorithm takes as input three variables :\n",
    "    C = cost_xy(x, y, scaling_coef)  # Wasserstein cost function\n",
    "\n",
    "    # both marginals are fixed with equal weights\n",
    "    mu = 1.0 / torch.tensor(n_data, dtype=torch.float32) * torch.ones(n_data, dtype=torch.float32)\n",
    "    nu = 1.0 / torch.tensor(n_data, dtype=torch.float32) * torch.ones(n_data, dtype=torch.float32)\n",
    "\n",
    "    # Parameters of the Sinkhorn algorithm.\n",
    "    thresh = 10**(-2)  # stopping criterion\n",
    "\n",
    "    # Elementary operations .....................................................................\n",
    "    def M(u, v):\n",
    "        '''\n",
    "        Modified cost for logarithmic updates\n",
    "        $M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\n",
    "        '''\n",
    "        return (-C + u[:,None] + v[None,:]) / epsilon\n",
    "\n",
    "    def lse(A):\n",
    "        '''\n",
    "        log-sum-exp\n",
    "        '''\n",
    "        return A.logsumexp(dim=1, keepdim=True)\n",
    "        # return tf.math.log(tf.reduce_sum(tf.exp(A), axis=1, keepdims=True) + 1e-6)  # add 10^-6 to prevent NaN\n",
    "\n",
    "    # Actual Sinkhorn loop ......................................................................\n",
    "    u, v, err = 0. * mu, 0. * nu, 0.\n",
    "\n",
    "    for i in range(L):\n",
    "        u1 = u  # useful to check the update\n",
    "        u = epsilon * (torch.log(mu) - torch.squeeze(lse(M(u, v)))) + u\n",
    "        #print(M(u, v).transpose)\n",
    "        v = epsilon * (torch.log(nu) - torch.squeeze(lse(torch.transpose(M(u, v), 0, 1)))) + v\n",
    "        err =torch.sum(torch.abs(u - u1))\n",
    "        if torch.greater(torch.tensor(thresh), err) and i >= Lmin:\n",
    "            break\n",
    "    U, V = u, v\n",
    "    pi = torch.exp(M(U, V))  # Transport plan pi = diag(a)*K*diag(b)\n",
    "    cost = torch.sum(pi * C)  # Sinkhorn cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "benchmark_sinkhorn(x, y, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
