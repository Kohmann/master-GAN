{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from timegan_cnn_DGER import *\n",
    "from utils import *\n",
    "from trainers import *\n",
    "\n",
    "\n",
    "trainset = TimeGANDatasetSinus(num=600, seq_len=100, features=3, temporal=True)\n",
    "testset = TimeGANDatasetSinus(num=300, seq_len=100, features=3, temporal=True)\n",
    "print(len(trainset))\n",
    "print(trainset[0][0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/kohmann/timeGAN/e/TIMEGAN-99\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "\n",
      "Start Embedding Network Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.0499: 100%|██████████| 800/800 [00:56<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training with Supervised Loss Only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 544, Loss: 0.0718:  68%|██████▊   | 544/800 [00:45<00:20, 12.45it/s]"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"kohmann/timeGAN\",\n",
    "    name=\"timegan_cnn_DGER\",\n",
    "    tags=[\"cnn\",\"DGER\" \"batchnorm\", \"temporal\"],\n",
    "    description=\"DGER temporal test\",\n",
    "    source_files=[\"timegan_cnn_DGER.py\"],\n",
    "    capture_hardware_metrics=False,\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3YjFjNGY5MS1kOWU1LTRmZjgtOTNiYS0yOGI2NDdjZGYzNWUifQ==\",\n",
    ")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "params = {\n",
    "    \"model_name\": \"model_cnn_dger.pt\",\n",
    "    \"n_epochs\": 800,\n",
    "    \"l_rate\": 0.0001,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"hidden_dim\": 20,\n",
    "    \"Z_dim\": 100,\n",
    "    \"dis_thresh\": 0.15,\n",
    "    \"dataset\": \"sinus\",\n",
    "    \"feature_dim\": trainset[0][0].size(1),\n",
    "    \"max_seq_len\": trainset[0][0].size(0),\n",
    "    \"trainset_size\": len(trainset),\n",
    "    \"device\": device,\n",
    "    \"optimizer\": \"Adam\",\n",
    "}\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TimeGAN(feature_dim=params[\"feature_dim\"],\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                padding_value=0.,\n",
    "                Z_dim=params[\"Z_dim\"],\n",
    "                max_seq_len=params[\"max_seq_len\"],\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                device=torch.device(params[\"device\"]))\n",
    "\n",
    "timegan_trainer(model,\n",
    "                trainset,\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                device=torch.device(params[\"device\"]),\n",
    "                learning_rate=params[\"l_rate\"],\n",
    "                n_epochs=params[\"n_epochs\"],\n",
    "                max_seq_len=params[\"max_seq_len\"],\n",
    "                dis_thresh=params[\"dis_thresh\"],\n",
    "                neptune_logger=run,\n",
    "                model_name=params[\"model_name\"]\n",
    "                )\n",
    "# Generate random synthetic data\n",
    "gen_z = timegan_generator(model, torch.tensor(testset.T), params[\"model_name\"], torch.device(device), testset[0][0].size(0), 100)\n",
    "\n",
    "\n",
    "r = np.array([data[0].numpy() for data in testset])\n",
    "f_pca = visualization(r, gen_z, 'pca')\n",
    "run[\"PCA\"].upload(f_pca)\n",
    "plt.close(f_pca)\n",
    "f_tsne = visualization(r, gen_z, 'tsne')\n",
    "run[\"tsne\"].upload(f_tsne)\n",
    "plt.close(f_tsne)\n",
    "\n",
    "run[\"mode_collapse\"] = modeCollapseEvaluator(r, gen_z)\n",
    "run[\"model_checkpoint\"].upload(params[\"model_name\"])\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-17T17:13:14.417930Z",
     "iopub.status.busy": "2022-10-17T17:13:14.417102Z",
     "iopub.status.idle": "2022-10-17T17:13:18.361982Z",
     "shell.execute_reply": "2022-10-17T17:13:18.361253Z",
     "shell.execute_reply.started": "2022-10-17T17:13:14.417890Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gen_z = timegan_generator(model, torch.tensor(testset.T), \"model_cnn_dg_meta.pt\", torch.device(device), testset[0][0].size(0),\n",
    "                          100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-17T17:14:31.295255Z",
     "iopub.status.busy": "2022-10-17T17:14:31.294906Z",
     "iopub.status.idle": "2022-10-17T17:14:31.422590Z",
     "shell.execute_reply": "2022-10-17T17:14:31.421789Z",
     "shell.execute_reply.started": "2022-10-17T17:14:31.295229Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample = 12\n",
    "plt.plot(gen_z[sample][:, 0], label=\"gen_sin1\")\n",
    "plt.plot(gen_z[sample][:, 1], label=\"gen_sin2\")\n",
    "plt.plot(gen_z[sample][:, 2], label=\"gen_sin3\")\n",
    "plt.title(\"Generated sampels\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-17T17:13:36.361464Z",
     "iopub.status.busy": "2022-10-17T17:13:36.360570Z",
     "iopub.status.idle": "2022-10-17T17:13:36.472153Z",
     "shell.execute_reply": "2022-10-17T17:13:36.470942Z",
     "shell.execute_reply.started": "2022-10-17T17:13:36.361401Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample = 1\n",
    "real_combo = create_sin3(gen_z[sample][:,0], gen_z[sample][:,1], temporal=True)\n",
    "real_comb_scaled = minmaxscaler().fit_transform(torch.tensor([real_combo]))[0]\n",
    "gen_combo = gen_z[sample][:,2]\n",
    "plt.plot(real_comb_scaled, label=\"true sim3\")\n",
    "plt.plot(gen_combo, label=\"generated sim3\")\n",
    "plt.title(\"sim3 - true vs generated\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = trainset.__getitem__(5)\n",
    "plt.plot(sample[0][:,0], label=\"sin1\")\n",
    "plt.plot(sample[0][:,1], label=\"sin2\")\n",
    "plt.plot(sample[0][:,2], label=\"(sin1 + sin2)/2\")\n",
    "plt.title(\"Real data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_comb = create_sin3(gen_z[sample][:,0], gen_z[sample][:,0], temporal=True)\n",
    "real_comb_scaled = minmaxscaler().fit_transform(torch.tensor([real_combo]))\n",
    "plt.plot(real_comb_scaled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN layer testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def printLayerShapes(layers, input):\n",
    "    for layer in layers:\n",
    "        output = layer(input)\n",
    "        print(f\"{type(layer).__name__}: {list(input.size())} --> {list(output.size())}\")\n",
    "        input = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d: [20, 100, 20] --> [20, 20, 8]\n",
      "BatchNorm1d: [20, 20, 8] --> [20, 20, 8]\n",
      "LeakyReLU: [20, 20, 8] --> [20, 20, 8]\n",
      "Conv1d: [20, 20, 8] --> [20, 40, 1]\n",
      "BatchNorm1d: [20, 40, 1] --> [20, 40, 1]\n",
      "LeakyReLU: [20, 40, 1] --> [20, 40, 1]\n",
      "Flatten: [20, 40, 1] --> [20, 40]\n",
      "Linear: [20, 40] --> [20, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "input = torch.randn(20, 100, 20)\n",
    "\n",
    "\n",
    "## Discrininator\n",
    "layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=100, out_channels=20, kernel_size=(5), stride=2, bias=False)\n",
    "            #,nn.Flatten(start_dim=1)\n",
    "            ,nn.BatchNorm1d(20)\n",
    "            ,nn.LeakyReLU()\n",
    "            ,nn.Conv1d(in_channels=20, out_channels=40, kernel_size=(7), stride=2, bias=False)\n",
    "            #,nn.Flatten(start_dim=1)\n",
    "            ,nn.BatchNorm1d(40)\n",
    "            ,nn.LeakyReLU()\n",
    "            ,nn.Flatten(start_dim=1)\n",
    "            ,nn.Linear(40, 1)\n",
    "        )\n",
    "printLayerShapes(layers, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d: [20, 100, 100] --> [20, 100, 48]\n",
      "BatchNorm1d: [20, 100, 48] --> [20, 100, 48]\n",
      "LeakyReLU: [20, 100, 48] --> [20, 100, 48]\n",
      "Conv1d: [20, 100, 48] --> [20, 100, 20]\n"
     ]
    }
   ],
   "source": [
    "## Generator\n",
    "input = torch.randn(20, 100, 100)\n",
    "\n",
    "layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(6), stride=2, bias=False)\n",
    "            , nn.BatchNorm1d(100)\n",
    "            , nn.LeakyReLU()\n",
    "            , nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(9), stride=2, bias=True)\n",
    "        )\n",
    "\n",
    "printLayerShapes(layers, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d: [20, 100, 100] --> [20, 100, 48]\n",
      "BatchNorm1d: [20, 100, 48] --> [20, 100, 48]\n",
      "LeakyReLU: [20, 100, 48] --> [20, 100, 48]\n",
      "Conv1d: [20, 100, 48] --> [20, 100, 23]\n"
     ]
    }
   ],
   "source": [
    "# Recovery\n",
    "\n",
    "## Discrininator\n",
    "layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(5), stride=2, bias=False)\n",
    "            #,nn.Flatten(start_dim=1)\n",
    "            ,nn.BatchNorm1d(100)\n",
    "            ,nn.LeakyReLU()\n",
    "            ,nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3), stride=2, bias=True)\n",
    "\n",
    "        )\n",
    "printLayerShapes(layers, input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output = layers[0](input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvTranspose1d: [20, 100, 3] --> [20, 100, 10]\n",
      "BatchNorm1d: [20, 100, 10] --> [20, 100, 10]\n",
      "LeakyReLU: [20, 100, 10] --> [20, 100, 10]\n",
      "ConvTranspose1d: [20, 100, 10] --> [20, 100, 20]\n"
     ]
    }
   ],
   "source": [
    "## Embedder\n",
    "input = torch.randn(20, 100, 3)\n",
    "\n",
    "layers = nn.Sequential(\n",
    "            #nn.Conv1d(in_channels=100, out_channels=100, kernel_size=(3), stride=2, bias=False)\n",
    "            nn.ConvTranspose1d(in_channels=100, out_channels=100, kernel_size=(6), stride=2, bias=False)\n",
    "            , nn.BatchNorm1d(100)\n",
    "            , nn.LeakyReLU()\n",
    "            , nn.ConvTranspose1d(in_channels=100, out_channels=100, kernel_size=(2), stride=2, bias=False)\n",
    "        )\n",
    "\n",
    "printLayerShapes(layers, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d: [20, 100, 20] --> [20, 100, 8]\n",
      "BatchNorm1d: [20, 100, 8] --> [20, 100, 8]\n",
      "LeakyReLU: [20, 100, 8] --> [20, 100, 8]\n",
      "Conv1d: [20, 100, 8] --> [20, 100, 3]\n"
     ]
    }
   ],
   "source": [
    "## Recovery\n",
    "input = torch.randn(20, 100, 20)\n",
    "layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=100, out_channels=100, kernel_size=5, stride=2, bias=False)\n",
    "            , nn.BatchNorm1d(100)\n",
    "            , nn.LeakyReLU()\n",
    "            , nn.Conv1d(in_channels=100, out_channels=100, kernel_size=3, stride=2, bias=True)\n",
    "            #, nn.Flatten(start_)\n",
    "        )\n",
    "printLayerShapes(layers, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
